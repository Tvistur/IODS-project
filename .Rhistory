# note to self
1 + 1
1 + 1
1 + 1
2 + 1
2 + 1 # cooment after the line
# comment above the code
1 + 1
2 + 1 # cooment after the line
1 + 1
seq(2, 100, by = 2)
2 ^ 0.5
((6 + 2) * 4) ^ 2
6 + 2 * 4 ^ 2
assign("z", 100)
x <- 2
y <- 9
z <-  100
x;y;z
print(c(x,y,z))
average(x,y,z)
average(c(x,y,z)
ls()
ls()
if(require(swirl)) {
message("swirl loaded correctly")
} else {
install.packages("swirl")
}
install.packages("swirl")
swirl()
install.packages("swirl")
library("swirl")
swirl()
average <-  (test1 + test2) / 2
average <-  (test1 + test2) / 2
test1 <- 75
test2 <- 83
average <-  (test1 + test2) / 2
average
ls()
pi
pi <- 3,14
pi <- 3.14
pi
rm(pi)
pi
body(mean)
body(rm)
test_results <- C(72, 83, 77, 45, 100)
test_results <- c(72, 83, 77, 45, 100)
sum(test_results)
avg_test_results <- sum(test_results) / length(test_results)
avg_test_results2 <- mean(test_results)
trimmed_avg <- mean(test_results, trim = .2)
trimmed_avg
ordered_results <-  sort(test_results)
ordered_results
trimmed_avg2 <- mean(c(72, 77, 83))
trimmed_avg2
mean(test_results, .2)
installe.packages("magrittr")
install.packages("magrittr")
search()
install.packages("dplyr")
search()
library(dplyr)
rm(list = ls())
if(require(swirl)) {
message("swirl loaded correctly")
} else {
install.packages("package_name", dependences = TRUE)
}
library("swirl")
install_course()
install_course()
swirl()
2 + 2
getwd()
get()
getwd()
install.packages("rmarkdown")
1 + 1
getwd
getwd()
install.packages(c("forcats", "htmlTable", "Matrix", "nlme", "TH.data", "tidyverse", "xml2"))
learning2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt")
str(learning2014) # 184 observations and 60 variables; column names V1:V60
dim(learning2014) #184 rows and 60 columns
deep <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D07","D14","D22","D30")
surf <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
stra <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
library(dplyr)
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
learning2014 <- select(learning2014, one_of(keep_columns))
getwd()
setwd("~/Documents/IODS K17/IODS-project")
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
pairs(Boston)
cor_matrix <- cor(Boston) %>% round(digits = 2)
library(tidyr)
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
library(corrplot)
install.packages("corrplot")
library(corrplot)
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
summary(boston_scaled)
boston_scaled <-  scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
summary(boston_scaled$crim)
bins <- quantile(boston_scaled$crim)
bins
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
n <- nrow(boston_scaled)
ind <- sample(n, size = n*0.8)
train <- boston_scaled[ind,]
test <-  boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.fit <- lda(data = train, crime ~.)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
plot(lda.fit, dimen = 2, col = classes, pch =classes)
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch =classes)
lda.arrows(lda.fit, myscale = 1)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
test
str(test)
str(train)
str(test)
str(correct_classes)
data(Boston)
?load
data("Boston")
str(Boston)
boston_scaled2 <- scale(Boston)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
dist_man <- dist(boston_scaled2)
summary(dist_man)
dist_man <- dist(boston_scaled2, method = "manhattan")
summary(dist_man)
km <- kmeans(dist_eu, centers = 10)
pairs(boston_scaled2, col = km$cluster)
km_man <- kmeans(dist_man, centers = 10)
pairs(boston_scaled2, col = km_man$cluste)
pairs(boston_scaled2, col = km_man$cluster)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = "line")
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = "line")
km_eu <- kmeans(dist_eu, centers = 5)
pairs(boston_scaled2, col = km_eu$cluster)
k_max <- 5
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = "line") # drops at about 2
# Euclidean distance matrix
dist <- dist(boston_scaled2)
summary(dist)
km <- kmeans(dist, centers = 10)
pairs(boston_scaled2, col = km$cluster)
km <- kmeans(dist, centers = 2)
pairs(boston_scaled2, col = km$cluster)
# plot matrix
pairs(Boston)
# calculate correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)
# print correlation matrix
cor_matrix
# visualize coorelation matrix
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
# Chunk 1: setup ch 4
knitr::opts_chunk$set(include= TRUE,
tidy = FALSE,
size = "small",
out.width = "70%",
fig.align = "center",
fig.width = 10,
fig.asp = 0.618,
fig.show = "hold",
fig.path = "../figures/document/",
cache.path = "../cache/document/",
cache = TRUE,
par = TRUE,
collapse = TRUE,
echo = FALSE,
message = FALSE,
warning = FALSE
)
options(digits = 3)
options(table_counter = TRUE)
# Chunk 2: preliminaries ch4
library(MASS)
library(tidyr)
library(corrplot)
library(ggplot2)
# Chunk 3: import data ch 4
data("Boston")
# Chunk 4: exploration
str(Boston)
summary(Boston)
# Chunk 5: graphical overview
# plot matrix
pairs(Boston)
# calculate correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)
# print correlation matrix
cor_matrix
# visualize coorelation matrix
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
# Chunk 6: scale
# standardize variables
boston_scaled <-  scale(Boston)
summary(boston_scaled)
# check class of boston_scaled object
class(boston_scaled) # matrix
# change object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# Chunk 7: new categorical variable 'crime'
# summary of SCALED crime rate
summary(boston_scaled$crim)
# create quantile vector of 'crim' and print it
bins <- quantile(boston_scaled$crim)
bins
# create categorical variable crime
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
# look at the table of new variable
table(crime)
# remove original crim from dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add new categorical variable to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# Chunk 8: divide into train and test sets
# create object n for number of rows in Boston dataset
n <- nrow(boston_scaled)
# choose randomly 80% of rows
ind <- sample(n, size = n*0.8)
# create TRAIN set
train <- boston_scaled[ind,]
# create TEST set
test <-  boston_scaled[-ind,]
# Chunk 9: LDA
# fit LDA to train set
lda.fit <- lda(data = train, crime ~.)
# print lda.fit object
lda.fit
# arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot results
plot(lda.fit, dimen = 2, col = classes, pch =classes)
lda.arrows(lda.fit, myscale = 1)
# Chunk 10
# save correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# Chunk 11: predict LDA
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# crosstabulate
table(correct = correct_classes, predicted = lda.pred$class)
# Chunk 12: reload data
# reload Boston dataset
data("Boston")
# standardize
boston_scaled2 <- scale(Boston)
# Chunk 13: distances
# Euclidean distance matrix
dist <- dist(boston_scaled2)
summary(dist)
# Chunk 14: k-means
# k-means clustering / Euclidean
km <- kmeans(dist, centers = 10)
# Chunk 15: optimal
# determine number of clusters
k_max <- 5
# calculate WCSS
twcss <- sapply(1:k_max, function(k){kmeans(dist, k)$tot.withinss})
# optimalm number of clusters
qplot(x = 1:k_max, y = twcss, geom = "line") # drops at 2
# run k-means again with optimal number of clusters
km <- kmeans(dist, centers = 2)
# visualize with optimal number of clusters
pairs(boston_scaled2, col = km$cluster)
getwd()
knitr::opts_chunk$set(include= TRUE,
tidy = FALSE,
size = "small",
out.width = "70%",
fig.align = "center",
fig.width = 10,
fig.height = 10,
fig.show = "hold",
fig.path = "../figures/document/",
cache.path = "../cache/document/",
cache = TRUE,
par = TRUE,
collapse = TRUE,
echo = FALSE,
message = FALSE,
warning = FALSE
)
options(digits = 3)
options(table_counter = TRUE)
knitr::opts_chunk$set(include= TRUE,
tidy = FALSE,
size = "small",
out.width = "70%",
fig.align = "center",
fig.show = "hold",
fig.path = "../figures/document/",
cache.path = "../cache/document/",
cache = TRUE,
par = TRUE,
collapse = TRUE,
echo = FALSE,
message = FALSE,
warning = FALSE
)
options(digits = 3)
options(table_counter = TRUE)
library(MASS)
library(tidyr)
library(corrplot)
library(ggplot2)
data("Boston")
dim(Boston)
dim(Boston)
str(Boston)
# plot matrix
pairs(Boston, col = "royalblue")
# calculate correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)
# print correlation matrix
cor_matrix
# visualize coorelation matrix
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
# standardize variables
boston_scaled <-  scale(Boston)
# check class of boston_scaled object
class(boston_scaled) # matrix
# change object to data frame
boston_scaled <- as.data.frame(boston_scaled)
summary(Boston$age)
summary(boston_scaled$age)
summary(Boston$tax)
summary(boston_scaled$tax)
summary(Boston$black)
summary(boston_scaled$black)
summary(boston_scaled$crim)
# create quantile vector of 'crim' and print it
bins <- quantile(boston_scaled$crim)
bins
# create object crime
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
## remove original crim from dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add new categorical variable to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# create object n for number of rows in Boston dataset
n <- nrow(boston_scaled)
# choose randomly 80% of rows
ind <- sample(n, size = n*0.8)
# create TRAIN set
train <- boston_scaled[ind,]
# create TEST set
test <-  boston_scaled[-ind,]
# fit LDA to train set
lda.fit <- lda(data = train, crime ~.)
# print lda.fit object
lda.fit
# arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch =classes)
lda.arrows(lda.fit, myscale = 1)
# save correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# crosstabulate
table(correct = correct_classes, predicted = lda.pred$class) %>%
addmargins()
# reload Boston dataset
data("Boston")
# standardize
boston_scaled2 <- scale(Boston)
# Euclidean distance matrix using set.seed()
set.seed(123)
dist <- dist(boston_scaled2)
summary(dist)
# k-means clustering / Euclidean
km <- kmeans(dist, centers = 10)
# determine number of clusters
k_max <- 10
# calculate WCSS
twcss <- sapply(1:k_max, function(k){kmeans(dist, k)$tot.withinss})
# optimalm number of clusters
qplot(x = 1:k_max, y = twcss, geom = "line") # drops at 2
# run k-means again with optimal number of clusters
km <- kmeans(dist, centers = 2)
# visualize with optimal number of clusters
pairs(boston_scaled2, col = km$cluster)
#reload Boston
data("Boston")
boston_scaled3 <- scale(Boston)
set.seed(123)
dist <- dist(boston_scaled2)
km <- kmeans(dist, centers = 5)
pairs(boston_scaled2, col = km$cluster)
pairs(boston_scaled3, col = km$cluster)
pairs(boston_scaled, col = km$cluster)
lda.fit <- lda(data = train, km$cluster ~.)
lda.fit <- lda(data = train, km ~.)
lda.fit <- lda(data = boston_scaled, km$cluster ~.)
lda.fit
km <- kmeans(dist, centers = 4)
lda.fit <- lda(data = boston_scaled, km$cluster ~.)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch =classes)
lda.arrows(lda.fit, myscale = 1)
pairs(boston_scaled3, col = km$cluster)
boston_scaled3 <- scale(Boston)
lda.fit <- lda(data = boston_scaled3, km$cluster ~.)
# calculate correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)
# print correlation matrix
cor_matrix
knitr::opts_chunk$set(include= TRUE,
tidy = FALSE,
size = "small",
out.width = "70%",
fig.align = "center",
fig.width = '10',
fig.height = '10',
fig.show = "hold",
fig.path = "../figures/document/",
cache.path = "../cache/document/",
cache = TRUE,
par = TRUE,
collapse = TRUE,
echo = FALSE,
message = FALSE,
warning = FALSE
)
options(digits = 3)
options(table_counter = TRUE)

# Clustering and classification


```{r setup ch 4, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(include= TRUE,
  tidy = FALSE,     
  size = "small",   
  out.width = "70%",
  fig.align = "center",
  fig.width = 10,
  fig.height = 10,  
  fig.show = "hold", 
  fig.path = "../figures/document/",
  cache.path = "../cache/document/",
  cache = TRUE,
  par = TRUE,
  collapse = TRUE,
  echo = FALSE,
  message = FALSE, 
  warning = FALSE
)
options(digits = 3)
options(table_counter = TRUE)
```

```{r preliminaries ch4}
library(MASS)
library(tidyr)
library(corrplot)
library(ggplot2)
```

```{r import data ch 4}
data("Boston")
```


Clustering and classification are visual ways of exploring statistical data. 
In clustering, the aim is to group a set of objects in such a way that objects in the same group, i.e. cluster, are more similar (in some sense or another) to each other than to those in other groups clusters.

#### Overview of the data

This week I am using a built-in dataset, namely the Boston dataset which contains housing information in the Boston Mass are. The data has been collected by the U.S Census Service and it is also available [online](http://lib.stat.cmu.edu/datasets/boston).

```{r dimensions Boston, include=FALSE}
dim(Boston)
```

The Boston dataset consists of just 506 observations and there are 14 variables. Let us take a closer look at what those variables are.

```{r structure Boston}
str(Boston)
```

The variable names are not exactly self-explanatory. Because our analysis will again depend upon them, I will briefly explain what they are.

Variable | Description
---|------------------------------------------------
crim | per capita crime rate by town
zn | proportion of residential land zoned for lots over 25,000 sq.ft.
indus | proportion of non-retail business acres per town
chas | Charles River dummy variable (1 if tract bounds river; 0 otherwise)
nox | nitric oxides concentration (parts per 10 million)
rm | average number of rooms per dwelling
age | proportion of owner-occupied units built prior to 1940
dis | weighted distances to five Boston employment centres
rad | index of accessibility to radial highways
tax | full-value property-tax rate per $10,000
ptratio | pupil-teacher ratio by town
black | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
lstat | % lower status of the population
medv | median value of owner-occupied homes in $1000's

Thus, the Boston dataset consists of a variety of variables: demographic, economic, and environmental factors as well as safety.  
Let us next take a graphical tour of the data. In the figure below, each variable is plotted against the other variables.

```{r pairs}
# plot matrix
pairs(Boston, col = "royalblue")
```

Here we see some interesting distribution patterns, such as  
* accumulation close to the edges and corners of the box - e.g. *age/lstat*  
* diagonal shapes - e.g. *nox/dis*  
* compact round shapes close to one of the corners - e.g. *rad/tax*  
* binary positionas in all pairs for variables  *chas* and *rad*.  
  
Another way of approaching the relationships between variables is to look at their correlations. Below is a **correlation** matrix where ...

```{r correlation matrix, include=FALSE}
# calculate correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)

# print correlation matrix
cor_matrix
```

```{r visualization of correlation matrix}
# visualize coorelation matrix
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

DESCRIBE AND INTERPRET THE OUTPUTS (COMMENT ON DISTRIBUTIONS OF THE VARIABLES & RELATIONSHIPS BETWEEN THEM)





*4 Standardization*
standardize  dataset + summary:
```{r scale}
# standardize variables
boston_scaled <-  scale(Boston)

summary(Boston)
summary(boston_scaled)

# check class of boston_scaled object
class(boston_scaled) # matrix

# change object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```
HOW DID THE VARIABLES CHANGE?

create categorical variable of crime rate (from scaled crime rate), quantiles as breakpoints:
```{r new categorical variable crime}
# summary of SCALED crime rate
summary(boston_scaled$crim)

# create quantile vector of 'crim' and print it
bins <- quantile(boston_scaled$crim)
bins

# create categorical variable crime
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of new variable
table(crime)

# remove original crim from dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add new categorical variable to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

*Division into train and test sets*
divide dataset into train and test so that 80% of data belongs to train set:
```{r divide into train and test sets}
# create object n for number of rows in Boston dataset
n <- nrow(boston_scaled)

# choose randomly 80% of rows
ind <- sample(n, size = n*0.8)

# create TRAIN set
train <- boston_scaled[ind,]

# create TEST set
test <-  boston_scaled[-ind,]
```

*Linear discriminant analysis (LDA)*
fit linear discriminant analysis to train set (categorical crime rate as target variable & all others as predictor variables) + draw LDA (bi)plot:
```{r LDA}
# fit LDA to train set
lda.fit <- lda(data = train, crime ~.)

# print lda.fit object
lda.fit

# arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot results
plot(lda.fit, dimen = 2, col = classes, pch =classes)
lda.arrows(lda.fit, myscale = 1)
```

save crime categories from the test set:
```{r}
# save correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

*Prediction*
predit the classes with LDA model on test data + crosstabulate with crime categories from the test set (= correct_classes):
```{r predict LDA}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# crosstabulate
table(correct = correct_classes, predicted = lda.pred$class)
```
>> COMMENT ON RESULTS

*K-means clustering*
reload Boston + standardize
```{r reload data}
# reload Boston dataset
data("Boston")

# standardize
boston_scaled2 <- scale(Boston)
```

calculate distances between observations:
```{r distances}
# Euclidean distance matrix
dist <- dist(boston_scaled2)
summary(dist)
```

run k-means clustering [Euclidean]:
```{r k-means}
# k-means clustering / Euclidean
km <- kmeans(dist, centers = 10)
```

investigate optimal number of clusters & run k-means again:
```{r optimal}
# determine number of clusters
k_max <- 5

# calculate WCSS
twcss <- sapply(1:k_max, function(k){kmeans(dist, k)$tot.withinss})

# optimalm number of clusters
qplot(x = 1:k_max, y = twcss, geom = "line") # drops at 2

# run k-means again with optimal number of clusters
km <- kmeans(dist, centers = 2)

# visualize with optimal number of clusters
pairs(boston_scaled2, col = km$cluster)
```
INTERPRET RESULTS

  
#### 3D plot

We were also given the code for producing a 3D plot.  I did not have time to study it in detail, but I absolutely had to see what what it looks like, so here it is!
```{r super-bonus, echo=FALSE}
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

library(plotly)
```

check colours (should be the crime classes of the train set)
```{r 3D plot}
# 3D plot of columns of the matrix product
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = classes)
```

